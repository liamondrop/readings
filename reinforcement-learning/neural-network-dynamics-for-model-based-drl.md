# Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning

Sample efficiency is an ever-present concern when considering most model-free RL algorithms. In principle, model-based RL should learn more efficiently, but it has been challenging to extend this deep neural networks. This paper demonstrates the use of "medium-sized" neural nets, combined with Model Predictive Control (MPC) to achieve dramatically improved sample efficiency.

Procedure:
  1. Collect a dataset and use it to train a dynamics model.
  2. This dynamics model is then used to run an MPC controller to execute a task.
  3. The trajectories generated by the execution of the controller are added to the dataset.
  4. Using both the old and new data, the dynamics model is retrained.
  5. Go back to step 2. until the desired performance is achieved.

Additionally, the paper discusses using this model-based approach as a way to initialize a model-free learner, using the generality of model-free learning to refine the model-based weights. Intuitively, this means rapidly guiding the weights to a promising region using a known model like MPC and then using slower model-free learning to "burn in" the task-/agent-specific weights for something like optimal performance.
